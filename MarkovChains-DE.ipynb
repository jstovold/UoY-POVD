{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains (local version)\n",
    "Department of Computer Science    \n",
    "University of York\n",
    "\n",
    "Authors: Dr David Zendle and Dr James Stovold\n",
    "\n",
    "Download original file from https://github.com/jstovold/UoY-POVD\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_U37PJ7aLyf"
   },
   "source": [
    "## Introduction \n",
    "\n",
    "This is a Jupyter notebook.\n",
    "\n",
    "It is an interactive environment that lets you both write and execute python scripts in a virtual environment.\n",
    "\n",
    "If you look to the left of the page, there should be a blue or green vertical bar. This shows you which chunk of code is currently selected. \n",
    "\n",
    "If you click on the following chunk of code, you will see that the vertical bar moves down. Click the 'Run' button at the top of the screen to execute that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QD210CeajW6"
   },
   "outputs": [],
   "source": [
    "print(\"for instance, pressing run while this code is selected will print this text out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to change the code, ensure the bar on the left is green by clicking in the code itself, then you are able to make changes. Why not try this out by changing the code below to output the number 5 instead of 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQz5jLKIdpPT"
   },
   "outputs": [],
   "source": [
    "i=10\n",
    "print(\"or running this code will print out the number\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvqIuIyxanha"
   },
   "source": [
    "***\n",
    "Markov chains are mathematical systems that experience changes from one state to another according to certain probabilistic rules. Markov chains have lots of practical uses: For example, they can be used for predictive text on a mobile device.\n",
    "\n",
    "I've set up this notebook to show you how markov chains can be used to generate text. You can learn how this works by stepping through the program, reading each piece of text and pressing the 'play' symbol next to the associated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDw_QnUIbNhB"
   },
   "source": [
    "When you've seen how the code works, there are a few challenges for you: (N.B. The following Gutenberg links won't work from Germany)\n",
    "\n",
    "1. Can you get it to generate text from a different book? At the moment, the text generator is working from Jane Austen's Pride and Prejudice, located in a .txt file at Project Gutenberg ('[http://www.gutenberg.org/ebooks/42671.txt.utf-8](http://www.gutenberg.org/ebooks/42671.txt.utf-8)').  However, it should theoretically be able to generate text from any source file on the internet - for example Dracula ('[http://www.gutenberg.org/ebooks/345.txt.utf-8](http://www.gutenberg.org/ebooks/345.txt.utf-8)') or something called 'Astounding Stories of Super-Science' ('[https://www.gutenberg.org/ebooks/29768.txt.utf-8](https://www.gutenberg.org/ebooks/29768.txt.utf-8)'). I'm using old books because you can find them easily online (copyright free) from Project Gutenberg (e.g. [https://www.gutenberg.org/wiki/Science_Fiction_(Bookshelf)](https://www.gutenberg.org/wiki/Science_Fiction_(Bookshelf) )). But you could use anything.\n",
    "\n",
    "2. Can you get it to work with something bigger or smaller than a 3-gram? Say a 4-gram or a 2-gram?\n",
    "\n",
    "3. Can you get it to create a mash-up of two books? This might involve creating a second bookInput from a different url, and then stitching the resulting strings together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ou0SVuGDAlcp"
   },
   "source": [
    "## Now, for the code itself...\n",
    "\n",
    "First, load a text file from Project Gutenberg to use as input for our text generator.\n",
    "\n",
    "The URL here points to a .txt file of Jane Austen's Pride and Prejudice.\n",
    "\n",
    "That URL is downloaded, decoded, and stored in a string variable called ```bookInput```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Germany-specific information\n",
    "\n",
    "Due to a copyright lawsuit (over 18 books!) Project Gutenberg is not accessible from within Germany. To get round this problem, the following piece of code will load Pride and Prejudice into memory from a local copy. If you wish to change the book used, Dracula is available at 'dracula.txt' and Astounding Stories... is available at 'astounding.txt'. \n",
    "\n",
    "It is worth highlighting that the books we are using are not part of the 18 books in the lawsuit, and are free to use worldwide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_6AS0M17kXg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import urllib\n",
    "import textwrap\n",
    "# non-germany-specific version:\n",
    "# bookInput = urllib.request.urlopen('http://www.gutenberg.org/ebooks/42671.txt.utf-8').read().decode('utf-8')\n",
    "# print(bookInput)\n",
    "\n",
    "# germany-specific version:\n",
    "f = open('pride.txt', 'r')\n",
    "bookInput = f.read()\n",
    "print(bookInput)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7YNbXIFg37"
   },
   "source": [
    "Now, the next thing we want to do is split our file up into tokens that we can use for our Markov chain's 'states'. \n",
    "\n",
    "The easiest way to do this is just to split our book up into words using python's 'split' command. This creates a list of each word in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0dUjCXHGSM6"
   },
   "outputs": [],
   "source": [
    "wordList = bookInput.split()\n",
    "print(\"The number of words in the book are:\",(len(wordList)))\n",
    "print(wordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8y0RMUg3Hycb"
   },
   "source": [
    "The next thing we want to do is take that big list, and transform it into a Markov Chain representation.\n",
    "\n",
    "Remember, Markov Chains are systems that shift from states according to probabilistic rules.\n",
    "\n",
    "So, if we had the sequence: A B B C, we would know that A->B 100% of the time; B->B 50% of the time, B->C 50% of the time. We want to form a similar probabilistic model here, but using words. Something that tells us \"If the current word is Pride, there is a 80% chance of going to the word 'and', a 10% chance of going to the word 'of', etc. etc. etc.\"\n",
    "\n",
    "The simplest way to do this is a dictionary: This will be a data structure that holds a mapping between each word, and all the words that ever follow that word. For example, if we had the sentence \"A sailor went to sea to see what he could see\", it would become the following dictionary:\n",
    "\n",
    "----------------\n",
    "A->sailor\n",
    "\n",
    "sailor->went\n",
    "\n",
    "went->to\n",
    "\n",
    "to->sea,see\n",
    "\n",
    "sea->to\n",
    "\n",
    "what->he\n",
    "\n",
    "he->could\n",
    "\n",
    "could->see\n",
    "\n",
    "------------------\n",
    "\n",
    "First, we create a set from each unique word in our list.\n",
    "\n",
    "Then, we use that set as keys to a dictionary, with each word linked to a (currently) empty list of 'following' words, which is denoted in python as '[]'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suExK7C_Hx51"
   },
   "outputs": [],
   "source": [
    "wordSet = set(wordList)\n",
    "print(\"There are this many unique words in the list: \",len(wordSet))\n",
    "wordDictionary = dict((word,[]) for word in wordSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jIekgmz0MWW_"
   },
   "source": [
    "Now, it's time to propogate the predictive model embodied in our dictionary with the actual data from our book.\n",
    "\n",
    "We use a 'for' loop to step through our list of words. For each word in our book, we record the next word after it in our dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qt38z-_eMxnG"
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(wordList)-1):\n",
    "    currentWord = wordList[i]\n",
    "    nextWord = wordList[i+1]\n",
    "    wordDictionary[currentWord].append(nextWord)\n",
    "    \n",
    "print(\"Model is built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaZ7wr9GRV4k"
   },
   "source": [
    "Let's see if it works! What words come after the word 'Darcy'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3HsH6zqRKqW"
   },
   "outputs": [],
   "source": [
    "if \"Darcy\" in wordDictionary:\n",
    "    print(wordDictionary[\"Darcy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWGcka6WReoj"
   },
   "source": [
    "Here's where the fun begins. We've successfully built a predictive model: Now let's have it try to generate some text.\n",
    "\n",
    "Let's start with a random word from our list, and randomly pick one of the words that it links to; then for that word, let's randomly pick one of the words that *it* links to; and so on!\n",
    "\n",
    "The only tricky bit of code here is building in a safeguard for a situation where our current word doesn't link to anything else: If this is the case, we break out of the 'for' loop and end our story early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kCqRVxiDRyzN"
   },
   "outputs": [],
   "source": [
    "story = \"\"\n",
    "word = random.choice(wordList)\n",
    "for i in range (0,100):\n",
    "    story = story +\" \"+ word\n",
    "    possibleNextWords = wordDictionary[word]\n",
    "    if len(possibleNextWords)==0:\n",
    "        break\n",
    "    word = random.choice(possibleNextWords)\n",
    "\n",
    "for line in textwrap.wrap(story):\n",
    "    print(line)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QESrGJsOUDub"
   },
   "source": [
    "So, that's kind of generating text. It's not very coherent, though.\n",
    "\n",
    "How can we improve our generative model?\n",
    "\n",
    "A quick improvement can be made via leveraging something called an n-gram: Basically, by chunking our input into fewer states, we can make our output obey implicit grammatical rules.\n",
    "\n",
    "For example, the input \"The cat sat in the hat\" could be represented as:\n",
    "\n",
    "---------\n",
    "The->cat\n",
    "\n",
    "cat->sat\n",
    "\n",
    "sat->in\n",
    "\n",
    "in->the\n",
    "\n",
    "the->hat\n",
    "\n",
    "---------\n",
    "\n",
    "Or it could be represented as the following 2-grams:\n",
    "\n",
    "---------\n",
    "\n",
    "The cat->sat in\n",
    "\n",
    "cat sat->in the\n",
    "\n",
    "sat in->the hat\n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "Note how the second contains implicit knowledge about things like articles of speech (e.g. 'the hat')\n",
    "\n",
    "We could even represent this through 3-grams:\n",
    "\n",
    "---------\n",
    "\n",
    "The cat sat -> in the hat\n",
    "\n",
    "---------\n",
    "\n",
    "Note that the larger your gram size, though, the more data you need to build your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7n7A0W14VKJy"
   },
   "source": [
    "Let's take our word list input, and transform it into a list of 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9-aWt4xVzgO"
   },
   "outputs": [],
   "source": [
    "gramSize = 3\n",
    "gramList=[]\n",
    "\n",
    "for i in range(0,len(wordList)-(gramSize-1)):\n",
    "    gram = wordList[i]\n",
    "    for j in range(1,gramSize):\n",
    "        gram = gram+\" \"+wordList[i+j]\n",
    "    gramList.append(gram)  \n",
    "\n",
    "print(gramList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1I2DWM-OX14M"
   },
   "source": [
    "Now we can use that list of 3-grams to create a set of 3-grams, and a dictionary of 3-grams, just like we did with individuals words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvtANHX1X-qX"
   },
   "outputs": [],
   "source": [
    "gramSet = set(gramList)\n",
    "print(\"There are this many unique grams in the list: \",len(gramSet))\n",
    "gramDictionary = dict((gram,[]) for gram in gramSet)\n",
    "\n",
    "for i in range(0,len(gramList)-gramSize):\n",
    "    currentGram = gramList[i]\n",
    "    nextGram = gramList[i+gramSize]\n",
    "    gramDictionary[currentGram].append(nextGram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jrpKVDpYgW-"
   },
   "source": [
    "Now, let's try generating some text from this model using exactly the same code as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxxBzSO8YyWV"
   },
   "outputs": [],
   "source": [
    "story = \"\"\n",
    "gram = random.choice(gramList)\n",
    "for i in range (0,100):\n",
    "    story = story +\" \"+ gram\n",
    "    possibleNextGrams = gramDictionary[gram]\n",
    "    if len(possibleNextGrams)==0:\n",
    "        break\n",
    "    gram = random.choice(possibleNextGrams)\n",
    "\n",
    "for line in textwrap.wrap(story):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7qgkDoAKZlcY"
   },
   "source": [
    "Much more coherent!\n",
    "\n",
    "Now, if you're interested, go to the top and do the challenges.\n",
    "\n",
    "Note that if you want to run all the code again, you don't have to step through every single line.\n",
    "\n",
    "The menu at the top (Cell->Run All) will just run all the code for you! You can then just scroll to the bottom to see the end product. You might have to wait a little while for the computation to complete, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MarkovChains.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
